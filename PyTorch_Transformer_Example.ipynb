{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzL1iTNRXxo28h7uGfNc/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azilya/torch_tutorials/blob/main/PyTorch_Transformer_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a basic transformer model to better understand its structure"
      ],
      "metadata": {
        "id": "LP10e9El172s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ak1kMSqUx6Ft"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic building blocks: multi-head attention, positional embeddings, position-wise ff"
      ],
      "metadata": {
        "id": "8r6_O7tZ2FU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The main point of transformer models: computes attention between input tokens (how much each token influences the others).\n",
        "Multiple heads can compute multiple attentions for the same input, focusing on different features\n",
        "\n",
        "The forward method of the torch module does the following:\n",
        "1. reshapes input according to the number of heads,\n",
        "1. splits input into query, key and value vectors,\n",
        "1. calculates scaled dot product attention\n",
        "1. reshapes output from multiple heads"
      ],
      "metadata": {
        "id": "JBLgFzcD2tdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, dim_model):\n",
        "    super().__init__()\n",
        "    assert dim_model % num_heads == 0, \"Model dimensions must be divisible by the number of heads\"\n",
        "\n",
        "    # init vector dimensions\n",
        "    self.dim_model = dim_model  # model dimensions\n",
        "    self.num_heads = num_heads  # number of heads\n",
        "    self.sub_dim = dim_model // num_heads  # dimensions of Q, K, V vectors\n",
        "\n",
        "    # init linear layers\n",
        "    self.W_q = nn.Linear(dim_model, dim_model)  # Q transform\n",
        "    self.W_k = nn.Linear(dim_model, dim_model)  # K transform\n",
        "    self.W_v = nn.Linear(dim_model, dim_model)  # V transform\n",
        "    self.W_o = nn.Linear(dim_model, dim_model)  # output transform\n",
        "\n",
        "  def scaled_dot_prod_attention(\n",
        "      self,\n",
        "      Q: torch.Tensor,\n",
        "      K: torch.Tensor,\n",
        "      V: torch.Tensor,\n",
        "      mask: torch.Tensor = None\n",
        "    ):\n",
        "    # calculate attention scores: multiply Q by K, and divide by root of sub-dimension\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.sub_dim)\n",
        "\n",
        "    # mask some of the inputs, if mask is provided, to improve predictions\n",
        "    if mask is not None:\n",
        "      attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
        "\n",
        "    # apply softmax to scores to obtain probabilities\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # multiply scores by V to obtain the final output\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x: torch.Tensor):\n",
        "    # reshape input by the number of attention heads\n",
        "    batch_size, seq_len, dim_model = x.size()\n",
        "    return x.view(batch_size, seq_len, self.num_heads, self.sub_dim).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x: torch.Tensor):\n",
        "    # recombine outputs from attention heads in a single tensor\n",
        "    # and make it contiguous for easier further use\n",
        "    batch_size, _, seq_len, sub_dim = x.size()\n",
        "    return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim_model)\n",
        "\n",
        "  def forward(self,\n",
        "      Q: torch.Tensor,\n",
        "      K: torch.Tensor,\n",
        "      V: torch.Tensor,\n",
        "      mask: torch.Tensor = None\n",
        "    ):\n",
        "    # apply transformations and split heads\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    # calculate attention scores\n",
        "    attn_output = self.scaled_dot_prod_attention(Q, K, V, mask)\n",
        "    # combine head outputs and apply final transformation\n",
        "    output = self.W_o(self.combine_heads(attn_output))\n",
        "    return output"
      ],
      "metadata": {
        "id": "ybmAk2KX2fRX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed-Forward Network\n",
        "\n",
        "Transformation, applied separately to each position of attention outputs. Two FF layers with a ReLU activation between them."
      ],
      "metadata": {
        "id": "tphybVWTH0Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFF(nn.Module):\n",
        "  def __init__(self, dim_model, dim_ff):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(dim_model, dim_ff)\n",
        "    self.layer2 = nn.Linear(dim_ff, dim_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer2(self.relu(self.layer1(x)))"
      ],
      "metadata": {
        "id": "j99aAJ3DHPtU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding\n",
        "\n",
        "Sine/cosine function, encoding each token's position in the input. This helps the model to consider relative positions of tokens in the sequence, in addition to their attention to each other."
      ],
      "metadata": {
        "id": "L1wV5rhQKM9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      dim_model: int,\n",
        "      max_seq_len: int = 4096,\n",
        "      dropout: float = 0.5\n",
        "    ):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    position = torch.arange(max_seq_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, dim_model, 2) * -(math.log(10_000.0) / dim_model))\n",
        "\n",
        "    pe = torch.zeros(max_seq_len, dim_model)\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # register as an untrainable parameter\n",
        "    self.register_buffer('pe', pe.unsquueze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.dropout(x + self.pe[:, :x.size(1)])"
      ],
      "metadata": {
        "id": "T_uPBs4oKMI_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Layer\n",
        "\n",
        "A single layer of the Transformer model, encoding the input. It encapsulates the multi-head attention block and the ff network, described above, adding a LayerNorm to each of their outputs. Thus the relations within the input can be efficiently encoded as embeddings, that can be used for other tasks.\n",
        "\n",
        "Generally models consist of several encoder layers, output of previous ones being fed in the following ones.\n",
        "\n",
        "Layer workings:\n",
        "1. Multi-head attention block\n",
        "1. Add + Layer normalization 1\n",
        "1. Position-wise feed-forward network\n",
        "1. Add + Layer normalization 2\n",
        "\n",
        "Dropout is applied to layer outputs before normalization."
      ],
      "metadata": {
        "id": "wPVbmEUJSMDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model: int, num_heads: int, dim_ff: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention = MultiHeadAttention(num_heads, dim_model)\n",
        "    self.ff = PositionWiseFF(dim_model, dim_ff)\n",
        "    self.norm1 = nn.LayerNorm(dim_model)\n",
        "    self.norm2 = nn.LayerNorm(dim_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    attn_output = self.self_attention(x, x, x, mask)\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "    ff_output = self.ff(x)\n",
        "    x = self.norm2(x + self.dropout(ff_output))\n",
        "    return x"
      ],
      "metadata": {
        "id": "qgPGr3dRQ8VC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model\n",
        "\n",
        "Several encoder layers can be stacked together to create an encoder transformer. This model produces a more meaningful embedding of the output than a single layer, which can be consequently used for downstream tasks, e.g. generation, classification etc."
      ],
      "metadata": {
        "id": "_P82bediBoF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size: int,\n",
        "      dim_model: int,\n",
        "      dim_ff: int,\n",
        "      n_heads: int,\n",
        "      n_layers: int,\n",
        "      dropout: float = 0.5\n",
        "    ):\n",
        "    super().__init_()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim_model)\n",
        "    self.pos_embedding = PositionalEmbedding(dim_model, dropout=dropout)\n",
        "    encoder_layer = EncoderLayer(dim_model, n_heads, dim_ff, dropout)\n",
        "    self.encoder_layers = nn.ModuleList([encoder_layer for _ in range(n_layers)])\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear = nn.Linear(dim_model, vocab_size)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "      initrange = 0.1\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "      self.linear.bias.data.zero_()\n",
        "      self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.dropout(self.pos_embedding(self.embedding(x)))\n",
        "    x_enc = x\n",
        "    for enc_layer in self.encoder_layers:\n",
        "      x_enc = enc_layer(x)\n",
        "    output = self.linear(x_enc)\n",
        "    return output"
      ],
      "metadata": {
        "id": "jVRK0w4wVKDe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The best part\n",
        "\n",
        "All Transformer parts can actually be just imported from Pytorch."
      ],
      "metadata": {
        "id": "SmKKMR_UH7dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, vocab_size: int, dim_model: int, dim_ff: int,\n",
        "               n_heads: int, n_layers: int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEmbedding(dim_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(dim_model, n_heads, dim_ff, dropout)\n",
        "    self.encoder_layers = TransformerEncoder(encoder_layers, n_layers)\n",
        "    self.embedding = nn.Embedding(vocab_size, dim_model)\n",
        "    self.dim_model = dim_model\n",
        "    self.linear = nn.Linear(dim_model, vocab_size)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "      initrange = 0.1\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "      self.linear.bias.data.zero_()\n",
        "      self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, x: torch.Tensor,\n",
        "              mask: torch.Tensor = None) -> torch.Tensor:\n",
        "      x = self.pos_embedding(self.embedding(x))\n",
        "      x_enc = self.encoder_layers(x, mask)\n",
        "      output = self.linear(x_enc)\n",
        "      return output"
      ],
      "metadata": {
        "id": "0p30RPBmH_PG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}